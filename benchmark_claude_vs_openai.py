#!/usr/bin/env python3
"""
Claude vs OpenAI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
í•œêµ­ì–´ ì²˜ë¦¬, ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„, í…œí”Œë¦¿ ìƒì„± ëŠ¥ë ¥ ì¢…í•© í‰ê°€
"""

import os
import time
import json
from typing import Dict, List, Any, Tuple
from dotenv import load_dotenv
import anthropic
import openai
from datetime import datetime
import statistics

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class AIModelComparison:
    """Claude vs OpenAI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self):
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.claude_client = anthropic.Anthropic(
            api_key=os.getenv("ANTHROPIC_API_KEY")
        )
        self.openai_client = openai.OpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        )

        self.results = {
            "test_date": datetime.now().isoformat(),
            "models_tested": {},
            "comparison_metrics": {},
            "detailed_analysis": {}
        }

    def test_claude_model(self, model_name: str, test_cases: List[Dict]) -> Dict[str, Any]:
        """Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\nClaude ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_name}")
        print("=" * 50)

        model_results = {
            "provider": "Anthropic",
            "model": model_name,
            "total_tests": len(test_cases),
            "passed_tests": 0,
            "failed_tests": 0,
            "average_response_time": 0,
            "total_tokens": 0,
            "test_results": []
        }

        total_time = 0
        total_tokens = 0

        for i, test_case in enumerate(test_cases, 1):
            print(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}: {test_case['name']}")

            try:
                start_time = time.time()

                response = self.claude_client.messages.create(
                    model=model_name,
                    max_tokens=test_case.get('max_tokens', 1000),
                    temperature=test_case.get('temperature', 0.3),
                    messages=[
                        {
                            "role": "user",
                            "content": test_case['prompt']
                        }
                    ]
                )

                end_time = time.time()
                response_time = end_time - start_time
                total_time += response_time

                # í† í° ì‚¬ìš©ëŸ‰
                input_tokens = response.usage.input_tokens
                output_tokens = response.usage.output_tokens
                total_test_tokens = input_tokens + output_tokens
                total_tokens += total_test_tokens

                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                quality_score = self.evaluate_response_quality(
                    test_case, response.content[0].text
                )

                test_result = {
                    "test_name": test_case['name'],
                    "category": test_case['category'],
                    "response_time": round(response_time, 2),
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": total_test_tokens,
                    "quality_score": quality_score,
                    "response_text": response.content[0].text,
                    "response_preview": response.content[0].text[:200] + "...",
                    "success": True
                }

                model_results["test_results"].append(test_result)
                model_results["passed_tests"] += 1

                print(f"  ì„±ê³µ - ì‘ë‹µì‹œê°„: {response_time:.2f}s, í† í°: {total_test_tokens}, í’ˆì§ˆ: {quality_score}/5")
                time.sleep(1)  # API ì œí•œ ê³ ë ¤

            except Exception as e:
                print(f"  ì‹¤íŒ¨: {str(e)}")
                model_results["test_results"].append({
                    "test_name": test_case['name'],
                    "category": test_case['category'],
                    "error": str(e),
                    "success": False
                })
                model_results["failed_tests"] += 1

        # í‰ê·  ê³„ì‚°
        if model_results["passed_tests"] > 0:
            model_results["average_response_time"] = round(total_time / model_results["passed_tests"], 2)
            model_results["total_tokens"] = total_tokens
            model_results["average_tokens_per_request"] = round(total_tokens / model_results["passed_tests"], 2)

        return model_results

    def test_openai_model(self, model_name: str, test_cases: List[Dict]) -> Dict[str, Any]:
        """OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\nOpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_name}")
        print("=" * 50)

        model_results = {
            "provider": "OpenAI",
            "model": model_name,
            "total_tests": len(test_cases),
            "passed_tests": 0,
            "failed_tests": 0,
            "average_response_time": 0,
            "total_tokens": 0,
            "test_results": []
        }

        total_time = 0
        total_tokens = 0

        for i, test_case in enumerate(test_cases, 1):
            print(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_cases)}: {test_case['name']}")

            try:
                start_time = time.time()

                response = self.openai_client.chat.completions.create(
                    model=model_name,
                    max_tokens=test_case.get('max_tokens', 1000),
                    temperature=test_case.get('temperature', 0.3),
                    messages=[
                        {
                            "role": "user",
                            "content": test_case['prompt']
                        }
                    ]
                )

                end_time = time.time()
                response_time = end_time - start_time
                total_time += response_time

                # í† í° ì‚¬ìš©ëŸ‰
                input_tokens = response.usage.prompt_tokens
                output_tokens = response.usage.completion_tokens
                total_test_tokens = input_tokens + output_tokens
                total_tokens += total_test_tokens

                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                response_text = response.choices[0].message.content
                quality_score = self.evaluate_response_quality(test_case, response_text)

                test_result = {
                    "test_name": test_case['name'],
                    "category": test_case['category'],
                    "response_time": round(response_time, 2),
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": total_test_tokens,
                    "quality_score": quality_score,
                    "response_text": response_text,
                    "response_preview": response_text[:200] + "...",
                    "success": True
                }

                model_results["test_results"].append(test_result)
                model_results["passed_tests"] += 1

                print(f"  ì„±ê³µ - ì‘ë‹µì‹œê°„: {response_time:.2f}s, í† í°: {total_test_tokens}, í’ˆì§ˆ: {quality_score}/5")
                time.sleep(1)  # API ì œí•œ ê³ ë ¤

            except Exception as e:
                print(f"  ì‹¤íŒ¨: {str(e)}")
                model_results["test_results"].append({
                    "test_name": test_case['name'],
                    "category": test_case['category'],
                    "error": str(e),
                    "success": False
                })
                model_results["failed_tests"] += 1

        # í‰ê·  ê³„ì‚°
        if model_results["passed_tests"] > 0:
            model_results["average_response_time"] = round(total_time / model_results["passed_tests"], 2)
            model_results["total_tokens"] = total_tokens
            model_results["average_tokens_per_request"] = round(total_tokens / model_results["passed_tests"], 2)

        return model_results

    def evaluate_response_quality(self, test_case: Dict, response: str) -> int:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ (1-5ì )"""
        score = 3  # ê¸°ë³¸ ì ìˆ˜

        if not response or len(response.strip()) < 20:
            return 1  # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ ì‘ë‹µ

        # ê¸¸ì´ ê¸°ë°˜ í‰ê°€
        if len(response) < 100:
            score -= 1
        elif len(response) > 500:
            score += 1

        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€
        category = test_case.get('category', '')

        if category == 'korean_generation':
            # í•œêµ­ì–´ ìƒì„± í’ˆì§ˆ í‰ê°€
            korean_indicators = ['ë‹˜', 'ìŠµë‹ˆë‹¤', 'í•´ì£¼ì„¸ìš”', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ì•ˆë…•í•˜ì„¸ìš”']
            korean_score = sum(1 for indicator in korean_indicators if indicator in response)
            score += min(2, korean_score // 2)

            # ë³€ìˆ˜ ì‚¬ìš© í™•ì¸
            if '#{' in response or '{' in response:
                score += 1

        elif category == 'policy_compliance':
            # ì •ì±… ì¤€ìˆ˜ í‰ê°€
            policy_terms = ['ì¤€ìˆ˜', 'ì •ì±…', 'ê·œì •', 'ê¸ˆì§€', 'í—ˆìš©', 'ì œí•œ']
            policy_score = sum(1 for term in policy_terms if term in response)
            score += min(2, policy_score // 2)

        elif category == 'business_analysis':
            # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ í‰ê°€
            business_terms = ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ì—…ì¢…', 'ì„œë¹„ìŠ¤', 'ë¶„ì„', 'ìœ í˜•', 'ì¹´í…Œê³ ë¦¬']
            business_score = sum(1 for term in business_terms if term in response)
            score += min(2, business_score // 2)

        elif category == 'korean_comprehension':
            # í•œêµ­ì–´ ì´í•´ë„ í‰ê°€
            comprehension_indicators = ['ë³€ê²½', 'ì•ˆë‚´', 'ì˜ˆì•½', 'ì¼ì •', 'ì‹œê°„']
            comp_score = sum(1 for indicator in comprehension_indicators if indicator in response)
            score += min(2, comp_score // 2)

        elif category == 'creativity':
            # ì°½ì˜ì„± í‰ê°€
            creative_elements = ['íŠ¹ë³„', 'ìƒˆë¡œìš´', 'ë…íŠ¹', 'ì°½ì˜', 'í˜ì‹ ']
            creative_score = sum(1 for element in creative_elements if element in response)
            score += min(1, creative_score // 2)

        elif category == 'long_context':
            # ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ í‰ê°€
            context_indicators = ['ì •ì±…', 'ê·œì •', 'ì¤€ìˆ˜', 'í…œí”Œë¦¿', 'ë³€ìˆ˜']
            context_score = sum(1 for indicator in context_indicators if indicator in response)
            score += min(1, context_score // 2)

        return min(5, max(1, score))

    def run_comprehensive_comparison(self):
        """í¬ê´„ì ì¸ ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("AI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘")
        print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)

        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
        test_cases = self.get_enhanced_test_cases()

        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
        models_to_test = [
            ("claude", "claude-3-5-haiku-latest"),
            ("claude", "claude-3-5-sonnet-latest"),
            ("openai", "gpt-4o-mini"),
            ("openai", "gpt-4o")
        ]

        for provider, model in models_to_test:
            try:
                if provider == "claude":
                    results = self.test_claude_model(model, test_cases)
                else:  # openai
                    results = self.test_openai_model(model, test_cases)

                self.results["models_tested"][f"{provider}_{model}"] = results

            except Exception as e:
                print(f"{provider} {model} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                self.results["models_tested"][f"{provider}_{model}"] = {"error": str(e)}

        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        self.perform_detailed_analysis()

        # ê²°ê³¼ ì €ì¥
        self.save_results()
        self.print_comprehensive_summary()

    def get_enhanced_test_cases(self) -> List[Dict]:
        """í–¥ìƒëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
        return [
            {
                "name": "í•œêµ­ì–´ ì•Œë¦¼í†¡ í…œí”Œë¦¿ ìƒì„±",
                "category": "korean_generation",
                "prompt": "ìŒì‹ì ì—ì„œ ì£¼ë¬¸ ì™„ë£Œ í›„ í”½ì—… ì•ˆë‚´ë¥¼ ìœ„í•œ ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ê³ ê°ëª…(#{ê³ ê°ëª…})ê³¼ í”½ì—…ì‹œê°„(#{í”½ì—…ì‹œê°„}) ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ê³ , ì •ì¤‘í•œ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.",
                "max_tokens": 800,
                "temperature": 0.3
            },
            {
                "name": "ì •ì±… ì¤€ìˆ˜ ê²€ì‚¬ ë° ê°œì„ ì•ˆ ì œì‹œ",
                "category": "policy_compliance",
                "prompt": "ë‹¤ìŒ ì•Œë¦¼í†¡ í…œí”Œë¦¿ì´ ì¹´ì¹´ì˜¤í†¡ ì •ì±…ì— ì¤€ìˆ˜í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³  ê°œì„ ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”: 'í• ì¸ì¿ í° ì§€ê¸ˆ ì£¼ë¬¸í•˜ë©´ 50% í• ì¸! í´ë¦­í•˜ì„¸ìš” ğŸ‘† ë§ˆê°ì„ë°•!'",
                "max_tokens": 1000,
                "temperature": 0.1
            },
            {
                "name": "ë³µí•© ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„",
                "category": "business_analysis",
                "prompt": "ì˜¨ë¼ì¸ êµìœ¡ í”Œë«í¼ì—ì„œ 'ìˆ˜ê°•ì‹ ì²­ ë§ˆê° ì—°ì¥ ì•ˆë‚´'ì™€ 'í™˜ë¶ˆ ì •ì±… ë³€ê²½ ê³µì§€'ë¥¼ ë™ì‹œì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ìƒí™©ì—ì„œ ì ì ˆí•œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµê³¼ ë©”ì‹œì§€ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                "max_tokens": 1200,
                "temperature": 0.2
            },
            {
                "name": "ì˜ë£Œì§„ ì „ë¬¸ ìš©ì–´ í¬í•¨ í•œêµ­ì–´ ì²˜ë¦¬",
                "category": "korean_comprehension",
                "prompt": "ì¢…í•©ë³‘ì›ì—ì„œ MRI ê²€ì‚¬ ì˜ˆì•½ì´ ë³€ê²½ë˜ì–´ í™˜ìì—ê²Œ ì•ˆë‚´í•´ì•¼ í•©ë‹ˆë‹¤. ê¸°ì¡´ ì˜ˆì•½(#{ê¸°ì¡´ë‚ ì§œ} #{ê¸°ì¡´ì‹œê°„}), ë³€ê²½ ì˜ˆì•½(#{ë³€ê²½ë‚ ì§œ} #{ë³€ê²½ì‹œê°„}), ë‹´ë‹¹ ì˜ë£Œì§„(#{ì˜ì‚¬ëª…} ì „ë¬¸ì˜)ì„ í¬í•¨í•˜ì—¬ ì˜ë£Œ í™˜ê²½ì— ì í•©í•œ ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì¸ í†¤ì˜ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "max_tokens": 900,
                "temperature": 0.2
            },
            {
                "name": "ì°½ì˜ì ì´ë©´ì„œ ì •ì±… ì¤€ìˆ˜í•˜ëŠ” ë§ˆì¼€íŒ… ë©”ì‹œì§€",
                "category": "creativity",
                "prompt": "í”„ë¦¬ë¯¸ì—„ ì¹´í˜ì—ì„œ ê³„ì ˆ í•œì • ì‹ ë©”ë‰´ 'ì œì£¼ ê°ê·¤ ë¼ë–¼'ë¥¼ ì¶œì‹œí•©ë‹ˆë‹¤. ê³ ê°ì˜ ê´€ì‹¬ì„ ëŒë©´ì„œë„ ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼í†¡ ì •ì±…ì„ ì™„ì „íˆ ì¤€ìˆ˜í•˜ëŠ” ì°½ì˜ì ì¸ ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ê³¼ë„í•œ ì´ëª¨ì§€ë‚˜ ê´‘ê³ ì„± ë¬¸êµ¬ ì—†ì´ ìš°ì•„í•˜ê³  ì„¸ë ¨ëœ í†¤ì„ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.",
                "max_tokens": 1000,
                "temperature": 0.4
            },
            {
                "name": "ë‹¤ì¤‘ ì •ì±… ë¬¸ì„œ ê¸°ë°˜ ë³µí•© í…œí”Œë¦¿ ìƒì„±",
                "category": "long_context",
                "prompt": """ë‹¤ìŒ ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼í†¡ ì •ì±…ë“¤ì„ ëª¨ë‘ ì¤€ìˆ˜í•˜ì—¬ ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ì£¼ë¬¸ í™•ì¸ í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

1. ê´‘ê³ ì„± ë¬¸êµ¬ ê¸ˆì§€ (í• ì¸, ì´ë²¤íŠ¸ ì§ì ‘ ì–¸ê¸‰ ì œí•œ)
2. ì´ëª¨ì§€ ìµœì†Œ ì‚¬ìš© (1-2ê°œ ì´ë‚´)
3. ë³€ìˆ˜ í˜•ì‹: #{ë³€ìˆ˜ëª…}
4. ëª…í™•í•œ ë°œì‹ ì ì •ë³´ í•„ìš”
5. ì¡´ëŒ“ë§ ì‚¬ìš© í•„ìˆ˜
6. ê°œì¸ì •ë³´ ë³´í˜¸ ì¤€ìˆ˜
7. ëª…í™•í•œ ì•ˆë‚´ ëª©ì  ëª…ì‹œ

í¬í•¨í•  ë³€ìˆ˜: #{ê³ ê°ëª…}, #{ì£¼ë¬¸ë²ˆí˜¸}, #{ìƒí’ˆëª…}, #{ë°°ì†¡ì˜ˆì •ì¼}, #{ì‡¼í•‘ëª°ëª…}""",
                "max_tokens": 1200,
                "temperature": 0.3
            },
            {
                "name": "ì‹¤ì‹œê°„ ìƒí™© ëŒ€ì‘ ë©”ì‹œì§€",
                "category": "real_time_response",
                "prompt": "íƒë°° ë°°ì†¡ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì§€ì—°ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê³ ê°ì—ê²Œ ìƒí™©ì„ íˆ¬ëª…í•˜ê²Œ ì•ˆë‚´í•˜ë©´ì„œë„ ë¶ˆì•ˆê°ì„ ìµœì†Œí™”í•˜ê³  ì‹ ë¢°ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì§€ì—° ì‚¬ìœ (#{ì§€ì—°ì‚¬ìœ }), ìƒˆë¡œìš´ ë°°ì†¡ ì˜ˆì •ì¼(#{ì‹ ë°°ì†¡ì¼}), ê³ ê°ëª…(#{ê³ ê°ëª…}) ë³€ìˆ˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.",
                "max_tokens": 900,
                "temperature": 0.25
            },
            {
                "name": "ë‹¤êµ­ì–´ ê³ ê° ëŒ€ìƒ í•œêµ­ì–´ ë©”ì‹œì§€",
                "category": "cross_cultural",
                "prompt": "í•œêµ­ì–´ë¥¼ ë°°ìš°ëŠ” ì™¸êµ­ì¸ ê³ ê°ë“¤ì´ ë§ì´ ì´ìš©í•˜ëŠ” ì˜¨ë¼ì¸ í•œêµ­ì–´ í•™ìŠµ í”Œë«í¼ì—ì„œ ìˆ˜ì—… ì¼ì • ë³€ê²½ì„ ì•ˆë‚´í•´ì•¼ í•©ë‹ˆë‹¤. í•œêµ­ì–´ ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ê°„ë‹¨ëª…ë£Œí•˜ë©´ì„œë„ ì •ì¤‘í•œ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
                "max_tokens": 800,
                "temperature": 0.2
            }
        ]

    def perform_detailed_analysis(self):
        """ìƒì„¸ ë¹„êµ ë¶„ì„ ìˆ˜í–‰"""
        print("\nìƒì„¸ ë¹„êµ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

        models = self.results["models_tested"]

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        performance_metrics = {}

        for model_key, model_data in models.items():
            if "error" in model_data:
                continue

            # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„
            category_performance = {}
            for test in model_data.get("test_results", []):
                if test["success"]:
                    category = test["category"]
                    if category not in category_performance:
                        category_performance[category] = {
                            "response_times": [],
                            "quality_scores": [],
                            "token_usage": []
                        }

                    category_performance[category]["response_times"].append(test["response_time"])
                    category_performance[category]["quality_scores"].append(test["quality_score"])
                    category_performance[category]["token_usage"].append(test["total_tokens"])

            # í†µê³„ ê³„ì‚°
            for category, data in category_performance.items():
                if data["response_times"]:
                    category_performance[category]["avg_response_time"] = statistics.mean(data["response_times"])
                    category_performance[category]["avg_quality"] = statistics.mean(data["quality_scores"])
                    category_performance[category]["avg_tokens"] = statistics.mean(data["token_usage"])

            performance_metrics[model_key] = {
                "overall": {
                    "avg_response_time": model_data.get("average_response_time", 0),
                    "avg_tokens": model_data.get("average_tokens_per_request", 0),
                    "success_rate": model_data.get("passed_tests", 0) / model_data.get("total_tests", 1)
                },
                "by_category": category_performance
            }

        self.results["comparison_metrics"] = performance_metrics

    def save_results(self):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"claude_vs_openai_comparison_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)

        print(f"\nê²°ê³¼ ì €ì¥ë¨: {filename}")

    def print_comprehensive_summary(self):
        """í¬ê´„ì ì¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("AI ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ê²°ê³¼")
        print("="*80)

        models = self.results["models_tested"]

        # ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        print("\nì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print("-" * 60)

        for model_key, model_data in models.items():
            if "error" in model_data:
                print(f"{model_key}: í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - {model_data['error']}")
                continue

            provider = model_data.get("provider", "Unknown")
            model_name = model_data.get("model", "Unknown")

            print(f"\n{provider} - {model_name}")
            print(f"  ì„±ê³µë¥ : {model_data['passed_tests']}/{model_data['total_tests']}")
            print(f"  í‰ê·  ì‘ë‹µì‹œê°„: {model_data['average_response_time']}ì´ˆ")
            print(f"  í‰ê·  í† í° ì‚¬ìš©: {model_data.get('average_tokens_per_request', 0)}")

            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_scores = [test['quality_score'] for test in model_data['test_results'] if test['success']]
            if quality_scores:
                avg_quality = statistics.mean(quality_scores)
                print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}/5")

        # ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        print(f"\nì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ì„±ëŠ¥:")
        print("-" * 60)

        categories = set()
        for model_data in models.values():
            if "test_results" in model_data:
                for test in model_data["test_results"]:
                    if test["success"]:
                        categories.add(test["category"])

        for category in categories:
            best_model = None
            best_score = 0

            for model_key, model_data in models.items():
                if "test_results" in model_data:
                    category_scores = [test['quality_score'] for test in model_data['test_results']
                                     if test['success'] and test['category'] == category]
                    if category_scores:
                        avg_score = statistics.mean(category_scores)
                        if avg_score > best_score:
                            best_score = avg_score
                            best_model = model_key

            if best_model:
                print(f"  {category}: {best_model} (ì ìˆ˜: {best_score:.2f})")

if __name__ == "__main__":
    comparison = AIModelComparison()
    comparison.run_comprehensive_comparison()