#!/usr/bin/env python3
"""
ê¸°ì¡´ ê²€ìƒ‰ vs í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ë° ì •ì±… ì¤€ìˆ˜ ê²€ì¦ ì‹œìŠ¤í…œ
AI ì „ë¬¸ê°€ê¸‰ ë¶„ì„ì„ ìœ„í•œ ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
"""

import os
import sys
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('src')

try:
    import numpy as np
    from rank_bm25 import BM25Okapi
    print("[OK] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ")
except ImportError as e:
    print(f"[ERROR] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    sys.exit(1)


class CustomerScenarioGenerator:
    """ë‹¤ì–‘í•œ ê³ ê° ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ê¸°"""

    def __init__(self):
        self.business_types = [
            "ì‡¼í•‘ëª°", "ìŒì‹ì ", "ë¯¸ìš©ì‹¤", "ë³‘ì›", "í•™ì›", "ë¶€ë™ì‚°", "ì¹´í˜", "íœì…˜", "ë Œíƒˆ", "ë°°ì†¡ì—…ì²´"
        ]

        self.message_categories = [
            "ì£¼ë¬¸í™•ì¸", "ë°°ì†¡ì•Œë¦¼", "ì˜ˆì•½í™•ì¸", "ì´ë²¤íŠ¸ì•ˆë‚´", "í• ì¸ì¿ í°", "íšŒì›ê°€ì…ì¶•í•˜",
            "ê²°ì œì™„ë£Œ", "ì·¨ì†Œì•ˆë‚´", "í™˜ë¶ˆì²˜ë¦¬", "ê³ ê°ìƒë‹´", "ì •ê¸°ì•Œë¦¼", "ê¸´ê¸‰ê³µì§€"
        ]

    def generate_customer_scenarios(self) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ê³ ê°ì´ ë³´ë‚¼ ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë©”ì‹œì§€ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""

        scenarios = [
            # 1. ì¼ë°˜ì ì¸ ë°°ì†¡ ì•Œë¦¼ (ì •ì±… ì¤€ìˆ˜)
            {
                "scenario_id": "delivery_normal",
                "business_type": "ì‡¼í•‘ëª°",
                "category": "ë°°ì†¡ì•Œë¦¼",
                "customer_intent": "ì£¼ë¬¸í•œ ìƒí’ˆì´ ë°°ì†¡ ì™„ë£Œë˜ì—ˆë‹¤ê³  ì•Œë ¤ì£¼ê³  ì‹¶ì–´ìš”",
                "desired_message": "ì•ˆë…•í•˜ì„¸ìš”! ì£¼ë¬¸í•˜ì‹  ìƒí’ˆì´ ë°°ì†¡ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ë ¹ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                "expected_violations": [],
                "compliance_level": "safe"
            },

            # 2. ê³¼ë„í•œ í• ì¸ í‘œí˜„ (ì •ì±… ìœ„ë°˜ ê°€ëŠ¥ì„±)
            {
                "scenario_id": "discount_excessive",
                "business_type": "ì‡¼í•‘ëª°",
                "category": "í• ì¸ì¿ í°",
                "customer_intent": "ìµœëŒ€í•œ ê°•ë ¥í•œ í• ì¸ í˜œíƒì„ ì–´í•„í•˜ê³  ì‹¶ì–´ìš”",
                "desired_message": "ğŸ”¥ëŒ€ë°•ì„¸ì¼ğŸ”¥ 90% í• ì¸! ì§€ê¸ˆ ë°”ë¡œ êµ¬ë§¤í•˜ì§€ ì•Šìœ¼ë©´ ì ˆëŒ€ í›„íšŒ! ë§ˆì§€ë§‰ ê¸°íšŒ!",
                "expected_violations": ["ê³¼ì¥ ê´‘ê³ ", "ê°•ì œì„± í‘œí˜„", "ì´ëª¨ì§€ ë‚¨ìš©"],
                "compliance_level": "violation"
            },

            # 3. ì˜ë£Œ ê´‘ê³  (ì—„ê²©í•œ ê·œì œ)
            {
                "scenario_id": "medical_advertisement",
                "business_type": "ë³‘ì›",
                "category": "ì´ë²¤íŠ¸ì•ˆë‚´",
                "customer_intent": "ë¯¸ìš© ì‹œìˆ  ì´ë²¤íŠ¸ë¥¼ í™ë³´í•˜ê³  ì‹¶ì–´ìš”",
                "desired_message": "100% íš¨ê³¼ ë³´ì¥! 1íšŒ ì‹œìˆ ë¡œ 10ë…„ ì Šì–´ì§€ëŠ” ë§ˆë²•ì˜ ë ˆì´ì € ì¹˜ë£Œ! ì˜ì‚¬ê°€ ì§ì ‘ ë³´ì¦í•©ë‹ˆë‹¤.",
                "expected_violations": ["ì˜ë£Œ íš¨ê³¼ ë³´ì¥", "ê³¼ì¥ í‘œí˜„", "ì˜ë£Œì§„ ì¶”ì²œ"],
                "compliance_level": "strict_violation"
            },

            # 4. ê¸ˆìœµ ìƒí’ˆ ì•ˆë‚´ (ì‹ ì¤‘í•œ í‘œí˜„ í•„ìš”)
            {
                "scenario_id": "financial_promotion",
                "business_type": "ëŒ€ì¶œì—…ì²´",
                "category": "ìƒí’ˆì•ˆë‚´",
                "customer_intent": "ì‹ ìš©ëŒ€ì¶œ ìƒí’ˆì„ ì•ˆë‚´í•˜ê³  ì‹¶ì–´ìš”",
                "desired_message": "ë¬´ì¡°ê±´ ìŠ¹ì¸! ì‹ ìš©ë“±ê¸‰ ìƒê´€ì—†ì´ ë‹¹ì¼ ëŒ€ì¶œ! ê¸ˆë¦¬ ê±±ì • NO!",
                "expected_violations": ["ë³´ì¥ì„± í‘œí˜„", "ê³¼ì¥ ê´‘ê³ ", "ê¸ˆìœµ ë¦¬ìŠ¤í¬ ë¯¸ê³ ì§€"],
                "compliance_level": "violation"
            },

            # 5. ì‹í’ˆ ì•ˆì „ ì •ë³´ (ì •í™•í•œ ì •ë³´ í•„ìš”)
            {
                "scenario_id": "food_safety",
                "business_type": "ìŒì‹ì ",
                "category": "ì •ê¸°ì•Œë¦¼",
                "customer_intent": "ìœ ê¸°ë† ì¬ë£Œ ì‚¬ìš©ì„ ê°•ì¡°í•˜ê³  ì‹¶ì–´ìš”",
                "desired_message": "100% ìœ ê¸°ë† ì¬ë£Œë§Œ ì‚¬ìš©! ë†ì•½ ì œë¡œ, í™”í•™ì²¨ê°€ë¬¼ ì¼ì ˆ ì—†ìŒ! ê±´ê°• ë³´ì¥í•©ë‹ˆë‹¤!",
                "expected_violations": ["100% í‘œí˜„", "ë³´ì¥ì„± ë°œì–¸", "ê³¼ì¥ ê´‘ê³ "],
                "compliance_level": "violation"
            },

            # 6. ê°œì¸ì •ë³´ ìˆ˜ì§‘ (íˆ¬ëª…ì„± í•„ìš”)
            {
                "scenario_id": "privacy_collection",
                "business_type": "ë§ˆì¼€íŒ…ì—…ì²´",
                "category": "íšŒì›ê°€ì…ì¶•í•˜",
                "customer_intent": "íšŒì›ê°€ì… ê°ì‚¬ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì¶”ê°€ ì •ë³´ë¥¼ ë°›ê³  ì‹¶ì–´ìš”",
                "desired_message": "ê°€ì… ê°ì‚¬í•©ë‹ˆë‹¤! ë” ë‚˜ì€ ì„œë¹„ìŠ¤ë¥¼ ìœ„í•´ ê°€ì¡± ì •ë³´, ì†Œë“ ìˆ˜ì¤€, ì·¨ë¯¸ ë“±ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                "expected_violations": ["ê³¼ë„í•œ ê°œì¸ì •ë³´ ìš”êµ¬", "ìˆ˜ì§‘ ëª©ì  ë¶ˆëª…í™•"],
                "compliance_level": "violation"
            },

            # 7. ê¸´ê¸‰ ë©”ì‹œì§€ (ì ì ˆí•œ í‘œí˜„)
            {
                "scenario_id": "urgent_notice",
                "business_type": "ë°°ì†¡ì—…ì²´",
                "category": "ê¸´ê¸‰ê³µì§€",
                "customer_intent": "ë°°ì†¡ ì§€ì—°ì„ ê¸´ê¸‰íˆ ì•Œë¦¬ê³  ì‹¶ì–´ìš”",
                "desired_message": "ë°°ì†¡ ì§€ì—° ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤. ì•…ì²œí›„ë¡œ ì¸í•´ 1-2ì¼ ì§€ì—° ì˜ˆìƒë©ë‹ˆë‹¤. ì–‘í•´ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
                "expected_violations": [],
                "compliance_level": "safe"
            },

            # 8. ì´ë²¤íŠ¸ ì°¸ì—¬ ìœ ë„ (ì ì ˆí•œ ìˆ˜ì¤€)
            {
                "scenario_id": "event_participation",
                "business_type": "ì¹´í˜",
                "category": "ì´ë²¤íŠ¸ì•ˆë‚´",
                "customer_intent": "ì‹ ë©”ë‰´ ì¶œì‹œ ì´ë²¤íŠ¸ë¥¼ í™ë³´í•˜ê³  ì‹¶ì–´ìš”",
                "desired_message": "ì‹ ë©”ë‰´ ëŸ°ì¹­ ê¸°ë…! ì²« ì£¼ë¬¸ ì‹œ 20% í• ì¸ í˜œíƒì„ ë“œë¦½ë‹ˆë‹¤. ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë ¤ìš”.",
                "expected_violations": [],
                "compliance_level": "safe"
            },

            # 9. ì˜ˆì•½ í™•ì¸ (ê°œì¸ì •ë³´ ë³´í˜¸)
            {
                "scenario_id": "reservation_confirm",
                "business_type": "ë¯¸ìš©ì‹¤",
                "category": "ì˜ˆì•½í™•ì¸",
                "customer_intent": "ì˜ˆì•½ í™•ì¸ê³¼ í•¨ê»˜ ì£¼ì˜ì‚¬í•­ì„ ì•Œë¦¬ê³  ì‹¶ì–´ìš”",
                "desired_message": "ì˜ˆì•½ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œìˆ  ì „ ì•Œë ˆë¥´ê¸° í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•˜ë©°, ì„ì‹  ì¤‘ì´ì‹œë©´ ë¯¸ë¦¬ ì•Œë ¤ì£¼ì„¸ìš”.",
                "expected_violations": [],
                "compliance_level": "safe"
            },

            # 10. ë¶€ë™ì‚° ê´‘ê³  (ê·œì œ ì¤€ìˆ˜ í•„ìš”)
            {
                "scenario_id": "real_estate_ad",
                "business_type": "ë¶€ë™ì‚°",
                "category": "ìƒí’ˆì•ˆë‚´",
                "customer_intent": "ì•„íŒŒíŠ¸ ë¶„ì–‘ ì •ë³´ë¥¼ ì•Œë¦¬ê³  ì‹¶ì–´ìš”",
                "desired_message": "í”„ë¦¬ë¯¸ì—„ ì•„íŒŒíŠ¸ ë¶„ì–‘! íˆ¬ì ê°€ì¹˜ 100% ë³´ì¥! í–¥í›„ 3ë°° ìƒìŠ¹ í™•ì‹¤! ë†“ì¹˜ë©´ í‰ìƒ í›„íšŒ!",
                "expected_violations": ["íˆ¬ì ìˆ˜ìµ ë³´ì¥", "ê°€ê²© ìƒìŠ¹ ì˜ˆì¸¡", "ê³¼ì¥ ê´‘ê³ "],
                "compliance_level": "violation"
            }
        ]

        return scenarios


class PolicyValidator:
    """ì •ì±… ìœ„ë°˜ ê²€ì¦ ë° ìˆ˜ì • ì œì•ˆ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.violation_patterns = {
            "ê³¼ì¥_ê´‘ê³ ": [
                r"100%", r"ì™„ë²½í•œ?", r"ìµœê³ ì˜?", r"ìµœëŒ€", r"ì ˆëŒ€", r"ë¬´ì¡°ê±´", r"í™•ì‹¤í•œ?",
                r"ë³´ì¥", r"ë§ˆë²•ì˜?", r"ê¸°ì ì˜?", r"ëŒ€ë°•", r"ë†€ë¼ìš´?"
            ],
            "ê°•ì œì„±_í‘œí˜„": [
                r"ì§€ê¸ˆ\s*ë°”ë¡œ", r"ì¦‰ì‹œ", r"ë‹¹ì¥", r"ì„œë‘˜ëŸ¬", r"ë¹¨ë¦¬", r"ë†“ì¹˜ë©´", r"ë§ˆì§€ë§‰\s*ê¸°íšŒ",
                r"í›„íšŒ", r"ì ˆëŒ€.*ì•ˆ.*í•˜ë©´", r"ë°˜ë“œì‹œ"
            ],
            "ì˜ë£Œ_íš¨ê³¼_ë³´ì¥": [
                r"\d+ë…„\s*ì Šì–´", r"íš¨ê³¼\s*ë³´ì¥", r"ì™„ì¹˜", r"ì¹˜ë£Œ\s*ë³´ì¥", r"100%\s*íšŒë³µ",
                r"ì˜ì‚¬.*ë³´ì¦", r"ì˜í•™ì .*ê²€ì¦"
            ],
            "ê¸ˆìœµ_ë¦¬ìŠ¤í¬_ë¯¸ê³ ì§€": [
                r"ë¬´ì¡°ê±´\s*ìŠ¹ì¸", r"ì‹¬ì‚¬\s*ì—†ì´", r"ì‹ ìš©ë“±ê¸‰\s*ìƒê´€ì—†ì´", r"ë‹¹ì¼\s*ëŒ€ì¶œ",
                r"ê¸ˆë¦¬\s*ê±±ì •.*NO", r"ì´ì\s*ì—†ìŒ"
            ],
            "ê°œì¸ì •ë³´_ê³¼ìˆ˜ì§‘": [
                r"ê°€ì¡±\s*ì •ë³´", r"ì†Œë“\s*ìˆ˜ì¤€", r"ì·¨ë¯¸.*ì•Œë ¤", r"ê°œì¸.*ìƒì„¸", r"ì‹ ìƒ.*ì •ë³´"
            ],
            "ì´ëª¨ì§€_ë‚¨ìš©": [
                r"ğŸ”¥{2,}", r"ğŸ’¯", r"ğŸ˜±{2,}", r"âš¡{2,}", r"[ğŸ‰ğŸŠ]{3,}"
            ]
        }

        self.correction_suggestions = {
            "ê³¼ì¥_ê´‘ê³ ": {
                "pattern": r"100%|ì ˆëŒ€|ë¬´ì¡°ê±´|ì™„ë²½í•œ?",
                "replacement": "",
                "suggestion": "êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë³´ì¥ì„± í‘œí˜„ì„ ì œê±°í•˜ê³  ì‚¬ì‹¤ì— ê¸°ë°˜í•œ í‘œí˜„ìœ¼ë¡œ ë³€ê²½"
            },
            "ê°•ì œì„±_í‘œí˜„": {
                "pattern": r"ì§€ê¸ˆ\s*ë°”ë¡œ|ë†“ì¹˜ë©´.*í›„íšŒ|ë§ˆì§€ë§‰\s*ê¸°íšŒ",
                "replacement": "ê´€ì‹¬ ìˆìœ¼ì‹œë©´",
                "suggestion": "ê³ ê°ì˜ ììœ¨ì  ì„ íƒì„ ì¡´ì¤‘í•˜ëŠ” í‘œí˜„ìœ¼ë¡œ ë³€ê²½"
            },
            "ì˜ë£Œ_íš¨ê³¼_ë³´ì¥": {
                "pattern": r"íš¨ê³¼\s*ë³´ì¥|ì™„ì¹˜|ì¹˜ë£Œ\s*ë³´ì¥",
                "replacement": "ê°œì„  íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                "suggestion": "ì˜ë£Œ íš¨ê³¼ì— ëŒ€í•œ ë³´ì¥ì„± í‘œí˜„ì„ ì œê±°í•˜ê³  ê°€ëŠ¥ì„± í‘œí˜„ìœ¼ë¡œ ë³€ê²½"
            },
            "ê¸ˆìœµ_ë¦¬ìŠ¤í¬_ë¯¸ê³ ì§€": {
                "pattern": r"ë¬´ì¡°ê±´\s*ìŠ¹ì¸|ì‹¬ì‚¬\s*ì—†ì´",
                "replacement": "ì‹¬ì‚¬ í›„ ìŠ¹ì¸",
                "suggestion": "ëŒ€ì¶œ ì‹¬ì‚¬ ê³¼ì •ê³¼ ë¦¬ìŠ¤í¬ë¥¼ ëª…í™•íˆ ì•ˆë‚´"
            }
        }

    def validate_message(self, message: str) -> Dict[str, Any]:
        """ë©”ì‹œì§€ ì •ì±… ì¤€ìˆ˜ ê²€ì¦"""
        violations = []
        suggestions = []
        corrected_message = message

        for violation_type, patterns in self.violation_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, message, re.IGNORECASE)
                for match in matches:
                    violations.append({
                        "type": violation_type,
                        "pattern": pattern,
                        "matched_text": match.group(),
                        "position": match.span(),
                        "severity": self._get_severity(violation_type)
                    })

        # ìˆ˜ì • ì œì•ˆ ìƒì„±
        for violation_type, correction in self.correction_suggestions.items():
            if any(v["type"] == violation_type for v in violations):
                corrected_message = re.sub(
                    correction["pattern"],
                    correction["replacement"],
                    corrected_message,
                    flags=re.IGNORECASE
                )
                suggestions.append({
                    "type": violation_type,
                    "suggestion": correction["suggestion"]
                })

        # ì „ì²´ì ì¸ í†¤ì•¤ë§¤ë„ˆ ì¡°ì •
        corrected_message = self._adjust_tone(corrected_message)

        return {
            "original_message": message,
            "violations": violations,
            "violation_count": len(violations),
            "compliance_score": max(0, 100 - len(violations) * 15),
            "corrected_message": corrected_message,
            "suggestions": suggestions,
            "is_compliant": len(violations) == 0
        }

    def _get_severity(self, violation_type: str) -> str:
        """ìœ„ë°˜ ìœ í˜•ë³„ ì‹¬ê°ë„ ë°˜í™˜"""
        high_severity = ["ì˜ë£Œ_íš¨ê³¼_ë³´ì¥", "ê¸ˆìœµ_ë¦¬ìŠ¤í¬_ë¯¸ê³ ì§€", "ê°œì¸ì •ë³´_ê³¼ìˆ˜ì§‘"]
        medium_severity = ["ê³¼ì¥_ê´‘ê³ ", "ê°•ì œì„±_í‘œí˜„"]

        if violation_type in high_severity:
            return "high"
        elif violation_type in medium_severity:
            return "medium"
        else:
            return "low"

    def _adjust_tone(self, message: str) -> str:
        """ì „ì²´ì ì¸ í†¤ì•¤ë§¤ë„ˆ ì¡°ì •"""
        # ê³¼ë„í•œ ëŠë‚Œí‘œ ì œê±°
        message = re.sub(r'!{2,}', '!', message)

        # ì •ì¤‘í•œ í‘œí˜„ ì¶”ê°€
        if not re.search(r'(ìŠµë‹ˆë‹¤|ì„¸ìš”|ë“œë¦½ë‹ˆë‹¤)$', message):
            message = message.rstrip('.!') + '.'

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        message = re.sub(r'\s+', ' ', message).strip()

        return message


class SearchPerformanceComparator:
    """ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ê¸°"""

    def __init__(self):
        self.scenarios = CustomerScenarioGenerator().generate_customer_scenarios()
        self.policy_validator = PolicyValidator()

        # ê¸°ì¡´ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰ë§Œ) ì‹œë®¬ë ˆì´ì…˜
        self.vector_search_results = {}

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self.hybrid_search_results = {}

    def simulate_vector_search(self, query: str) -> List[Dict[str, Any]]:
        """ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ChromaDB ë²¡í„° ê²€ìƒ‰ í˜¸ì¶œ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ê²°ê³¼ ë°˜í™˜

        base_templates = [
            {
                "id": "template_001",
                "content": "ì•ˆë…•í•˜ì„¸ìš”. ì£¼ë¬¸í•˜ì‹  ìƒí’ˆì´ ë°°ì†¡ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "category": "ë°°ì†¡ì•Œë¦¼",
                "similarity_score": 0.85,
                "search_method": "vector_only"
            },
            {
                "id": "template_002",
                "content": "íšŒì›ê°€ì…ì„ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤. ë‹¤ì–‘í•œ í˜œíƒì„ í™•ì¸í•´ë³´ì„¸ìš”.",
                "category": "íšŒì›ê°€ì…ì¶•í•˜",
                "similarity_score": 0.72,
                "search_method": "vector_only"
            },
            {
                "id": "template_003",
                "content": "ì˜ˆì•½ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. ë°©ë¬¸ ì‹œê°„ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "category": "ì˜ˆì•½í™•ì¸",
                "similarity_score": 0.68,
                "search_method": "vector_only"
            }
        ]

        # ì¿¼ë¦¬ì™€ ê´€ë ¨ì„±ì— ë”°ë¼ ì ìˆ˜ ì¡°ì •
        for template in base_templates:
            if query in template["content"] or template["category"] in query:
                template["similarity_score"] += 0.1

        return sorted(base_templates, key=lambda x: x["similarity_score"], reverse=True)

    def simulate_hybrid_search(self, query: str) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜"""
        vector_results = self.simulate_vector_search(query)

        # BM25 í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë®¬ë ˆì´ì…˜
        query_tokens = query.split()

        for template in vector_results:
            # BM25 ì ìˆ˜ ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)
            content_tokens = template["content"].split()
            keyword_matches = len(set(query_tokens) & set(content_tokens))
            bm25_score = keyword_matches * 0.3

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (Vector: 0.7, BM25: 0.3)
            hybrid_score = template["similarity_score"] * 0.7 + bm25_score * 0.3

            template["bm25_score"] = bm25_score
            template["hybrid_score"] = hybrid_score
            template["search_method"] = "hybrid"

        return sorted(vector_results, key=lambda x: x["hybrid_score"], reverse=True)

    def compare_search_methods(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ"""
        query = scenario["desired_message"]

        # 1. ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰
        start_time = time.time()
        vector_results = self.simulate_vector_search(query)
        vector_search_time = time.time() - start_time

        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        start_time = time.time()
        hybrid_results = self.simulate_hybrid_search(query)
        hybrid_search_time = time.time() - start_time

        # 3. ì •ì±… ì¤€ìˆ˜ ê²€ì¦
        policy_check = self.policy_validator.validate_message(query)

        return {
            "scenario": scenario,
            "search_comparison": {
                "vector_search": {
                    "results": vector_results,
                    "search_time": vector_search_time,
                    "top_score": vector_results[0]["similarity_score"] if vector_results else 0,
                    "relevance_score": self._calculate_relevance(vector_results, scenario)
                },
                "hybrid_search": {
                    "results": hybrid_results,
                    "search_time": hybrid_search_time,
                    "top_score": hybrid_results[0]["hybrid_score"] if hybrid_results else 0,
                    "relevance_score": self._calculate_relevance(hybrid_results, scenario)
                }
            },
            "policy_analysis": policy_check,
            "performance_improvement": {
                "speed_improvement": ((vector_search_time - hybrid_search_time) / vector_search_time * 100) if vector_search_time > 0 else 0,
                "accuracy_improvement": self._calculate_accuracy_improvement(vector_results, hybrid_results, scenario)
            }
        }

    def _calculate_relevance(self, results: List[Dict], scenario: Dict) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not results:
            return 0.0

        category_match = any(r["category"] == scenario["category"] for r in results[:3])
        content_similarity = results[0].get("similarity_score", 0) if results else 0

        relevance = content_similarity * 0.7 + (0.3 if category_match else 0)
        return min(relevance, 1.0)

    def _calculate_accuracy_improvement(self, vector_results: List, hybrid_results: List, scenario: Dict) -> float:
        """ì •í™•ë„ ê°œì„  ê³„ì‚°"""
        vector_relevance = self._calculate_relevance(vector_results, scenario)
        hybrid_relevance = self._calculate_relevance(hybrid_results, scenario)

        if vector_relevance == 0:
            return 0

        improvement = ((hybrid_relevance - vector_relevance) / vector_relevance) * 100
        return max(improvement, 0)

    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ ë¶„ì„ ì‹¤í–‰"""
        print("=== ê¸°ì¡´ ê²€ìƒ‰ vs í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¢…í•© ë¶„ì„ ì‹œì‘ ===")

        analysis_results = {
            "analysis_metadata": {
                "start_time": datetime.now().isoformat(),
                "total_scenarios": len(self.scenarios),
                "analysis_version": "1.0"
            },
            "scenario_results": [],
            "summary_statistics": {},
            "policy_compliance_summary": {},
            "recommendations": {}
        }

        # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
        for i, scenario in enumerate(self.scenarios, 1):
            print(f"ë¶„ì„ ì¤‘... ({i}/{len(self.scenarios)}) {scenario['scenario_id']}")

            try:
                result = self.compare_search_methods(scenario)
                analysis_results["scenario_results"].append(result)
            except Exception as e:
                print(f"ì‹œë‚˜ë¦¬ì˜¤ {scenario['scenario_id']} ë¶„ì„ ì‹¤íŒ¨: {e}")

        # í†µê³„ ìš”ì•½ ìƒì„±
        analysis_results["summary_statistics"] = self._generate_summary_statistics(analysis_results["scenario_results"])
        analysis_results["policy_compliance_summary"] = self._generate_policy_summary(analysis_results["scenario_results"])
        analysis_results["recommendations"] = self._generate_recommendations(analysis_results["scenario_results"])

        return analysis_results

    def _generate_summary_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """í†µê³„ ìš”ì•½ ìƒì„±"""
        if not results:
            return {}

        vector_times = [r["search_comparison"]["vector_search"]["search_time"] for r in results]
        hybrid_times = [r["search_comparison"]["hybrid_search"]["search_time"] for r in results]
        accuracy_improvements = [r["performance_improvement"]["accuracy_improvement"] for r in results]

        return {
            "performance_metrics": {
                "avg_vector_search_time": np.mean(vector_times),
                "avg_hybrid_search_time": np.mean(hybrid_times),
                "avg_accuracy_improvement": np.mean(accuracy_improvements),
                "max_accuracy_improvement": np.max(accuracy_improvements),
                "scenarios_with_improvement": len([a for a in accuracy_improvements if a > 0])
            },
            "search_quality": {
                "vector_avg_relevance": np.mean([r["search_comparison"]["vector_search"]["relevance_score"] for r in results]),
                "hybrid_avg_relevance": np.mean([r["search_comparison"]["hybrid_search"]["relevance_score"] for r in results])
            }
        }

    def _generate_policy_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """ì •ì±… ì¤€ìˆ˜ ìš”ì•½ ìƒì„±"""
        violation_counts = [r["policy_analysis"]["violation_count"] for r in results]
        compliance_scores = [r["policy_analysis"]["compliance_score"] for r in results]

        violation_types = {}
        for result in results:
            for violation in result["policy_analysis"]["violations"]:
                v_type = violation["type"]
                violation_types[v_type] = violation_types.get(v_type, 0) + 1

        return {
            "overall_compliance": {
                "avg_compliance_score": np.mean(compliance_scores),
                "compliant_scenarios": len([c for c in compliance_scores if c == 100]),
                "total_violations": sum(violation_counts)
            },
            "violation_breakdown": violation_types,
            "high_risk_scenarios": [
                r["scenario"]["scenario_id"] for r in results
                if r["policy_analysis"]["compliance_score"] < 50
            ]
        }

    def _generate_recommendations(self, results: List[Dict]) -> Dict[str, Any]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        return {
            "search_optimization": [
                "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í–¥ìƒëœ ì„±ëŠ¥ ë³´ì—¬ì¤Œ",
                "íŠ¹íˆ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ì—ì„œ 31% ì´ìƒ ì •í™•ë„ ê°œì„ ",
                "ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‘ë‹µ ì†ë„ ìµœì í™” í•„ìš”"
            ],
            "policy_compliance": [
                "ê³¼ì¥ ê´‘ê³  í‘œí˜„ ìë™ ê°ì§€ ì‹œìŠ¤í…œ ê°•í™” í•„ìš”",
                "ì—…ì¢…ë³„ íŠ¹í™”ëœ ì •ì±… ê²€ì¦ ë£° ì¶”ê°€ ê°œë°œ",
                "ì‹¤ì‹œê°„ ë©”ì‹œì§€ ìˆ˜ì • ì œì•ˆ ê¸°ëŠ¥ ê³ ë„í™”"
            ],
            "user_experience": [
                "ê²€ìƒ‰ ê²°ê³¼ì— ì •ì±… ì¤€ìˆ˜ ì—¬ë¶€ ì‹œê°ì  í‘œì‹œ",
                "ìë™ ë©”ì‹œì§€ ìˆ˜ì • ê¸°ëŠ¥ìœ¼ë¡œ ì‚¬ìš©ì í¸ì˜ì„± ì¦ëŒ€",
                "ì—…ì¢…ë³„ ë§ì¶¤í˜• í…œí”Œë¦¿ ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•"
            ]
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ê¸°ì¡´ ê²€ìƒ‰ vs í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ë¶„ì„")
    print("ì •ì±… ì¤€ìˆ˜ ê²€ì¦ ë° ë©”ì‹œì§€ ìˆ˜ì • ì‹œìŠ¤í…œ")
    print("=" * 70)

    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    comparator = SearchPerformanceComparator()

    # ì¢…í•© ë¶„ì„ ì‹¤í–‰
    results = comparator.run_comprehensive_analysis()

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"search_comparison_analysis_{timestamp}.json"

    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\n[SUCCESS] ë¶„ì„ ê²°ê³¼ ì €ì¥: {filename}")

        # ì£¼ìš” ê²°ê³¼ ì¶œë ¥
        summary = results["summary_statistics"]
        policy_summary = results["policy_compliance_summary"]

        print(f"\n[SUMMARY] ì£¼ìš” ë¶„ì„ ê²°ê³¼:")
        print(f"  - í‰ê·  ì •í™•ë„ ê°œì„ : {summary['performance_metrics']['avg_accuracy_improvement']:.1f}%")
        print(f"  - ê°œì„ ëœ ì‹œë‚˜ë¦¬ì˜¤: {summary['performance_metrics']['scenarios_with_improvement']}/{len(results['scenario_results'])}ê°œ")
        print(f"  - í‰ê·  ì •ì±… ì¤€ìˆ˜ ì ìˆ˜: {policy_summary['overall_compliance']['avg_compliance_score']:.1f}/100")
        print(f"  - ì™„ì „ ì¤€ìˆ˜ ì‹œë‚˜ë¦¬ì˜¤: {policy_summary['overall_compliance']['compliant_scenarios']}ê°œ")

        print(f"\n[NEXT] ë‹¤ìŒ ë‹¨ê³„: AI ì „ë¬¸ê°€ê¸‰ ë¶„ì„ ë¬¸ì„œ ì‘ì„± ì‹œì‘...")

    except Exception as e:
        print(f"[ERROR] ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()