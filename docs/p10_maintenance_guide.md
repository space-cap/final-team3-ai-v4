# P10. ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œ (Maintenance Guide)

## ğŸ”§ ìœ ì§€ë³´ìˆ˜ ê°œìš”

ì´ ë¬¸ì„œëŠ” ì¹´ì¹´ì˜¤ ì•Œë¦¼í†¡ í…œí”Œë¦¿ ìë™ ìƒì„± AI ì„œë¹„ìŠ¤ì˜ ì¥ê¸°ì ì¸ ìš´ì˜ì„ ìœ„í•œ ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œì…ë‹ˆë‹¤. ì •ê¸°ì ì¸ ì—…ë°ì´íŠ¸, ëª¨ë‹ˆí„°ë§, ë°±ì—…, ì„±ëŠ¥ ìµœì í™” ë“± ìš´ì˜ì— í•„ìš”í•œ ëª¨ë“  ì‚¬í•­ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ“… ì •ê¸° ìœ ì§€ë³´ìˆ˜ ì¼ì •

### ì¼ì¼ ì ê²€ (Daily)
- [ ] ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
- [ ] ë¡œê·¸ ëª¨ë‹ˆí„°ë§
- [ ] API ì‘ë‹µ ì‹œê°„ ì²´í¬
- [ ] ì—ëŸ¬ìœ¨ í™•ì¸

### ì£¼ê°„ ì ê²€ (Weekly)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„
- [ ] ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸
- [ ] ë””ìŠ¤í¬ ê³µê°„ ì •ë¦¬

### ì›”ê°„ ì ê²€ (Monthly)
- [ ] ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
- [ ] ë³´ì•ˆ íŒ¨ì¹˜ ì ìš©
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ë°±ì—… ë³µêµ¬ í…ŒìŠ¤íŠ¸

### ë¶„ê¸°ë³„ ì ê²€ (Quarterly)
- [ ] ì „ì²´ ì‹œìŠ¤í…œ ê°ì‚¬
- [ ] ì•„í‚¤í…ì²˜ ê²€í† 
- [ ] ìš©ëŸ‰ ê³„íš ìˆ˜ë¦½
- [ ] ì¬í•´ ë³µêµ¬ í›ˆë ¨

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§

#### í•µì‹¬ ì§€í‘œ (KPI)
```python
# monitoring/metrics_collector.py
import psutil
import time
from datetime import datetime
import json

class SystemMetrics:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""

    def collect_system_metrics(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent,
            "network_io": psutil.net_io_counters()._asdict(),
            "process_count": len(psutil.pids()),
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None
        }

    def collect_app_metrics(self):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # FastAPI ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Prometheus ì—°ë™)
        return {
            "requests_total": self.get_total_requests(),
            "response_time_avg": self.get_avg_response_time(),
            "error_rate": self.get_error_rate(),
            "active_connections": self.get_active_connections()
        }

    def save_metrics(self, metrics):
        """ë©”íŠ¸ë¦­ì„ íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(f"logs/metrics/{filename}", 'w') as f:
            json.dump(metrics, f, indent=2)
```

#### ì•Œë¦¼ ì„¤ì •
```python
# monitoring/alerting.py
import smtplib
from email.mime.text import MIMEText
import requests

class AlertManager:
    """ì•Œë¦¼ ê´€ë¦¬ì"""

    def __init__(self):
        self.thresholds = {
            "cpu_percent": 80,
            "memory_percent": 85,
            "disk_percent": 90,
            "error_rate": 5,  # 5% ì´ìƒ
            "response_time": 5000  # 5ì´ˆ ì´ìƒ
        }

    def check_alerts(self, metrics):
        """ì„ê³„ê°’ ì´ˆê³¼ í™•ì¸"""
        alerts = []

        for metric, threshold in self.thresholds.items():
            if metric in metrics and metrics[metric] > threshold:
                alerts.append({
                    "metric": metric,
                    "value": metrics[metric],
                    "threshold": threshold,
                    "severity": self.get_severity(metric, metrics[metric])
                })

        return alerts

    def send_alert(self, alert):
        """ì•Œë¦¼ ë°œì†¡"""
        if alert["severity"] == "critical":
            self.send_email_alert(alert)
            self.send_slack_alert(alert)
        else:
            self.send_slack_alert(alert)

    def send_slack_alert(self, alert):
        """Slack ì•Œë¦¼"""
        webhook_url = os.getenv("SLACK_WEBHOOK_URL")
        if not webhook_url:
            return

        message = {
            "text": f"ğŸš¨ {alert['metric']} Alert",
            "attachments": [{
                "color": "danger" if alert["severity"] == "critical" else "warning",
                "fields": [
                    {"title": "Metric", "value": alert["metric"], "short": True},
                    {"title": "Current Value", "value": str(alert["value"]), "short": True},
                    {"title": "Threshold", "value": str(alert["threshold"]), "short": True},
                    {"title": "Severity", "value": alert["severity"], "short": True}
                ]
            }]
        }

        requests.post(webhook_url, json=message)
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ê´€ë¦¬

#### ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •
```bash
# /etc/logrotate.d/kakao-template
/app/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 app app
    postrotate
        systemctl reload kakao-template
    endscript
}
```

#### ë¡œê·¸ ë¶„ì„ ìë™í™”
```python
# monitoring/log_analyzer.py
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter

class LogAnalyzer:
    """ë¡œê·¸ ë¶„ì„ê¸°"""

    def __init__(self, log_file):
        self.log_file = log_file

    def analyze_daily_logs(self, date=None):
        """ì¼ì¼ ë¡œê·¸ ë¶„ì„"""
        if date is None:
            date = datetime.now().date()

        stats = {
            "total_requests": 0,
            "error_count": 0,
            "avg_response_time": 0,
            "top_errors": Counter(),
            "hourly_distribution": defaultdict(int),
            "endpoint_usage": defaultdict(int)
        }

        response_times = []

        with open(self.log_file, 'r') as f:
            for line in f:
                if self.is_date_match(line, date):
                    stats["total_requests"] += 1

                    # ì‹œê°„ëŒ€ë³„ ë¶„í¬
                    hour = self.extract_hour(line)
                    if hour:
                        stats["hourly_distribution"][hour] += 1

                    # ì—ëŸ¬ ë¶„ì„
                    if "ERROR" in line:
                        stats["error_count"] += 1
                        error_type = self.extract_error_type(line)
                        stats["top_errors"][error_type] += 1

                    # ì‘ë‹µ ì‹œê°„ ë¶„ì„
                    response_time = self.extract_response_time(line)
                    if response_time:
                        response_times.append(response_time)

                    # ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©ëŸ‰
                    endpoint = self.extract_endpoint(line)
                    if endpoint:
                        stats["endpoint_usage"][endpoint] += 1

        if response_times:
            stats["avg_response_time"] = sum(response_times) / len(response_times)

        return stats

    def generate_report(self, stats):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""
# ì¼ì¼ ì‹œìŠ¤í…œ ë¦¬í¬íŠ¸ - {datetime.now().strftime('%Y-%m-%d')}

## ìš”ì•½
- ì´ ìš”ì²­ ìˆ˜: {stats['total_requests']:,}
- ì—ëŸ¬ ë°œìƒ: {stats['error_count']} ({stats['error_count']/stats['total_requests']*100:.2f}%)
- í‰ê·  ì‘ë‹µ ì‹œê°„: {stats['avg_response_time']:.2f}ms

## ìƒìœ„ ì—ëŸ¬
{self.format_top_errors(stats['top_errors'])}

## ì‹œê°„ëŒ€ë³„ ìš”ì²­ ë¶„í¬
{self.format_hourly_distribution(stats['hourly_distribution'])}

## ì¸ê¸° ì—”ë“œí¬ì¸íŠ¸
{self.format_endpoint_usage(stats['endpoint_usage'])}
        """
        return report
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### ìë™ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```python
# monitoring/performance_test.py
import time
import requests
import statistics
from concurrent.futures import ThreadPoolExecutor

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""

    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url

    def run_load_test(self, endpoint="/api/v1/templates/generate",
                     concurrent_users=10, duration=60):
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = {
            "start_time": time.time(),
            "end_time": None,
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "response_times": [],
            "errors": []
        }

        def make_request():
            try:
                start = time.time()
                response = requests.post(
                    f"{self.base_url}{endpoint}",
                    json={"user_request": "í…ŒìŠ¤íŠ¸ ìš”ì²­"},
                    timeout=10
                )
                end = time.time()

                response_time = (end - start) * 1000  # ms
                results["response_times"].append(response_time)
                results["total_requests"] += 1

                if response.status_code == 200:
                    results["successful_requests"] += 1
                else:
                    results["failed_requests"] += 1
                    results["errors"].append(f"HTTP {response.status_code}")

            except Exception as e:
                results["failed_requests"] += 1
                results["errors"].append(str(e))

        # ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            start_time = time.time()
            futures = []

            while time.time() - start_time < duration:
                if len(futures) < concurrent_users:
                    future = executor.submit(make_request)
                    futures.append(future)

                # ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                futures = [f for f in futures if not f.done()]
                time.sleep(0.1)

        results["end_time"] = time.time()
        return self.analyze_results(results)

    def analyze_results(self, results):
        """ê²°ê³¼ ë¶„ì„"""
        if not results["response_times"]:
            return results

        response_times = results["response_times"]
        results.update({
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p95_response_time": statistics.quantiles(response_times, n=20)[18],  # 95th percentile
            "rps": results["total_requests"] / (results["end_time"] - results["start_time"]),
            "success_rate": results["successful_requests"] / results["total_requests"] * 100
        })

        return results
```

## ğŸ”„ ë°±ì—… ë° ë³µêµ¬

### 1. ìë™ ë°±ì—… ì‹œìŠ¤í…œ

#### ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
```bash
#!/bin/bash
# scripts/backup_database.sh

BACKUP_DIR="/backups/kakao-template"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p $BACKUP_DIR

# Vector DB ë°±ì—…
echo "Backing up Vector Database..."
tar -czf $BACKUP_DIR/chroma_db_$DATE.tar.gz -C / app/chroma_db

# í…œí”Œë¦¿ ë°ì´í„° ë°±ì—…
echo "Backing up Template Data..."
cp /app/data/kakao_template_vectordb_data.json $BACKUP_DIR/template_data_$DATE.json

# ì„¤ì • íŒŒì¼ ë°±ì—…
echo "Backing up Configuration..."
tar -czf $BACKUP_DIR/config_$DATE.tar.gz -C /app \
    .env src/config/ requirements.txt

# ë¡œê·¸ ë°±ì—…
echo "Backing up Logs..."
tar -czf $BACKUP_DIR/logs_$DATE.tar.gz -C /app logs/

# ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
echo "Cleaning old backups..."
find $BACKUP_DIR -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete
find $BACKUP_DIR -name "*.json" -mtime +$RETENTION_DAYS -delete

echo "Backup completed: $DATE"
```

#### ìë™í™”ëœ ë°±ì—… ìŠ¤ì¼€ì¤„
```bash
# crontab -e
# ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ë°±ì—… ì‹¤í–‰
0 2 * * * /app/scripts/backup_database.sh

# ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œì— ì „ì²´ ë°±ì—…
0 3 * * 0 /app/scripts/full_backup.sh
```

### 2. ë³µêµ¬ ì ˆì°¨

#### ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# scripts/restore_database.sh

BACKUP_FILE=$1
RESTORE_DIR="/app"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    echo "Available backups:"
    ls -la /backups/kakao-template/
    exit 1
fi

echo "Stopping services..."
systemctl stop kakao-template

echo "Creating backup of current data..."
mv $RESTORE_DIR/chroma_db $RESTORE_DIR/chroma_db.bak.$(date +%Y%m%d_%H%M%S)

echo "Restoring from backup: $BACKUP_FILE"
tar -xzf $BACKUP_FILE -C /

echo "Setting permissions..."
chown -R app:app $RESTORE_DIR/chroma_db

echo "Starting services..."
systemctl start kakao-template

echo "Verifying restoration..."
sleep 10
curl -f http://localhost:8000/health || echo "Health check failed"

echo "Restoration completed"
```

### 3. ì¬í•´ ë³µêµ¬ ê³„íš

#### ë³µêµ¬ ì‹œê°„ ëª©í‘œ (RTO/RPO)
- **RTO (Recovery Time Objective)**: 4ì‹œê°„
- **RPO (Recovery Point Objective)**: 24ì‹œê°„
- **ì„œë¹„ìŠ¤ ê°€ìš©ì„± ëª©í‘œ**: 99.9%

#### ë³µêµ¬ ìš°ì„ ìˆœìœ„
1. **Critical**: í•µì‹¬ API ì„œë¹„ìŠ¤
2. **High**: ë°ì´í„°ë² ì´ìŠ¤ ë° ë°±ì—…
3. **Medium**: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
4. **Low**: ê°œë°œ ë„êµ¬ ë° ë¬¸ì„œ

## ğŸ”§ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸

### 1. ì˜ì¡´ì„± ì—…ë°ì´íŠ¸

#### ì •ê¸° ì—…ë°ì´íŠ¸ ì²´í¬
```python
# scripts/check_updates.py
import subprocess
import json
from datetime import datetime

def check_security_updates():
    """ë³´ì•ˆ ì—…ë°ì´íŠ¸ í™•ì¸"""
    try:
        result = subprocess.run(
            ["pip-audit", "--format", "json"],
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            vulnerabilities = json.loads(result.stdout)
            return vulnerabilities
        else:
            return {"error": result.stderr}
    except Exception as e:
        return {"error": str(e)}

def check_package_updates():
    """íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ í™•ì¸"""
    try:
        result = subprocess.run(
            ["pip", "list", "--outdated", "--format", "json"],
            capture_output=True,
            text=True
        )

        if result.returncode == 0:
            outdated = json.loads(result.stdout)
            return outdated
        else:
            return []
    except Exception as e:
        return []

def generate_update_report():
    """ì—…ë°ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    security_issues = check_security_updates()
    outdated_packages = check_package_updates()

    report = {
        "timestamp": datetime.now().isoformat(),
        "security_vulnerabilities": security_issues,
        "outdated_packages": outdated_packages,
        "recommendations": []
    }

    # ê¶Œì¥ì‚¬í•­ ìƒì„±
    if security_issues and "vulnerabilities" in security_issues:
        report["recommendations"].append("ğŸš¨ Critical: Security vulnerabilities found - immediate update required")

    critical_packages = ["fastapi", "uvicorn", "anthropic", "langchain"]
    for pkg in outdated_packages:
        if pkg["name"] in critical_packages:
            report["recommendations"].append(f"âš ï¸ Important: Update {pkg['name']} to {pkg['latest_version']}")

    return report

if __name__ == "__main__":
    report = generate_update_report()
    print(json.dumps(report, indent=2))
```

#### ì•ˆì „í•œ ì—…ë°ì´íŠ¸ ì ˆì°¨
```bash
#!/bin/bash
# scripts/safe_update.sh

PACKAGE_NAME=$1
TARGET_VERSION=$2

echo "ğŸ”„ Starting safe update process for $PACKAGE_NAME"

# 1. í˜„ì¬ ë²„ì „ ë°±ì—…
echo "ğŸ“¦ Backing up current environment..."
pip freeze > requirements.backup.$(date +%Y%m%d_%H%M%S).txt

# 2. í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
echo "ğŸ§ª Testing update in isolated environment..."
python -m venv test_env
source test_env/bin/activate
pip install $PACKAGE_NAME==$TARGET_VERSION

# 3. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
echo "ğŸ§ª Running basic tests..."
python -c "
import $PACKAGE_NAME
print(f'Successfully imported {$PACKAGE_NAME.__name__}')
"

if [ $? -eq 0 ]; then
    echo "âœ… Package test passed"
else
    echo "âŒ Package test failed"
    deactivate
    rm -rf test_env
    exit 1
fi

deactivate
rm -rf test_env

# 4. ì‹¤ì œ í™˜ê²½ ì—…ë°ì´íŠ¸
echo "ğŸ”„ Updating production environment..."
pip install $PACKAGE_NAME==$TARGET_VERSION

# 5. ì„œë¹„ìŠ¤ ì¬ì‹œì‘ ë° í—¬ìŠ¤ì²´í¬
echo "ğŸ”„ Restarting services..."
systemctl restart kakao-template

sleep 10

# 6. í—¬ìŠ¤ì²´í¬
echo "ğŸ” Performing health check..."
if curl -f http://localhost:8000/health; then
    echo "âœ… Update successful"
else
    echo "âŒ Health check failed - rolling back"
    pip install -r requirements.backup.$(ls -t requirements.backup.*.txt | head -1)
    systemctl restart kakao-template
    exit 1
fi

echo "ğŸ‰ Update completed successfully"
```

### 2. ì½”ë“œ ë°°í¬

#### ë¬´ì¤‘ë‹¨ ë°°í¬ (Blue-Green)
```bash
#!/bin/bash
# scripts/blue_green_deploy.sh

NEW_VERSION=$1
CURRENT_PORT=$(cat /app/current_port 2>/dev/null || echo "8000")
NEW_PORT=$((CURRENT_PORT == 8000 ? 8001 : 8000))

echo "ğŸš€ Starting Blue-Green deployment"
echo "Current: :$CURRENT_PORT, New: :$NEW_PORT"

# 1. ìƒˆ ë²„ì „ ë¹Œë“œ
echo "ğŸ”¨ Building new version..."
docker build -t kakao-template:$NEW_VERSION .

# 2. ìƒˆ ì»¨í…Œì´ë„ˆ ì‹œì‘
echo "ğŸƒ Starting new container on port $NEW_PORT..."
docker run -d \
    --name kakao-template-$NEW_PORT \
    -p $NEW_PORT:8000 \
    -e PORT=8000 \
    kakao-template:$NEW_VERSION

# 3. í—¬ìŠ¤ì²´í¬
echo "ğŸ” Health checking new version..."
sleep 30

for i in {1..10}; do
    if curl -f http://localhost:$NEW_PORT/health; then
        echo "âœ… New version is healthy"
        break
    fi

    if [ $i -eq 10 ]; then
        echo "âŒ New version failed health check"
        docker stop kakao-template-$NEW_PORT
        docker rm kakao-template-$NEW_PORT
        exit 1
    fi

    sleep 5
done

# 4. íŠ¸ë˜í”½ ì „í™˜ (ë¡œë“œ ë°¸ëŸ°ì„œ ì„¤ì • ë³€ê²½)
echo "ğŸ”„ Switching traffic to new version..."
# nginx ì„¤ì • ì—…ë°ì´íŠ¸
sed -i "s/:$CURRENT_PORT/:$NEW_PORT/g" /etc/nginx/sites-available/kakao-template
nginx -s reload

# 5. ì´ì „ ë²„ì „ ì •ë¦¬ (5ë¶„ í›„)
echo "â° Waiting 5 minutes before cleaning up old version..."
sleep 300

echo "ğŸ§¹ Cleaning up old version..."
docker stop kakao-template-$CURRENT_PORT
docker rm kakao-template-$CURRENT_PORT

# 6. í˜„ì¬ í¬íŠ¸ ì—…ë°ì´íŠ¸
echo $NEW_PORT > /app/current_port

echo "ğŸ‰ Blue-Green deployment completed successfully"
```

## ğŸ”’ ë³´ì•ˆ ìœ ì§€ë³´ìˆ˜

### 1. ë³´ì•ˆ ìŠ¤ìº”

#### ìë™ ë³´ì•ˆ ì ê²€
```python
# security/security_scanner.py
import subprocess
import json
import os
from datetime import datetime

class SecurityScanner:
    """ë³´ì•ˆ ìŠ¤ìºë„ˆ"""

    def scan_dependencies(self):
        """ì˜ì¡´ì„± ë³´ì•ˆ ìŠ¤ìº”"""
        try:
            result = subprocess.run(
                ["safety", "check", "--json"],
                capture_output=True,
                text=True
            )
            return json.loads(result.stdout) if result.stdout else {}
        except Exception as e:
            return {"error": str(e)}

    def scan_secrets(self):
        """ì‹œí¬ë¦¿ ìŠ¤ìº”"""
        try:
            result = subprocess.run(
                ["truffleHog", ".", "--json"],
                capture_output=True,
                text=True
            )
            return {"secrets_found": len(result.stdout.split('\n')) if result.stdout else 0}
        except Exception as e:
            return {"error": str(e)}

    def check_file_permissions(self):
        """íŒŒì¼ ê¶Œí•œ í™•ì¸"""
        critical_files = [
            ".env",
            "data/",
            "logs/",
            "chroma_db/"
        ]

        issues = []
        for file_path in critical_files:
            if os.path.exists(file_path):
                stat_info = os.stat(file_path)
                permissions = oct(stat_info.st_mode)[-3:]

                # .env íŒŒì¼ì€ 600 ê¶Œí•œì´ì–´ì•¼ í•¨
                if file_path == ".env" and permissions != "600":
                    issues.append(f"{file_path}: {permissions} (should be 600)")

                # ë””ë ‰í† ë¦¬ëŠ” 755 ê¶Œí•œì´ì–´ì•¼ í•¨
                elif os.path.isdir(file_path) and permissions not in ["755", "750"]:
                    issues.append(f"{file_path}: {permissions} (should be 755)")

        return issues

    def generate_security_report(self):
        """ë³´ì•ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "dependency_scan": self.scan_dependencies(),
            "secret_scan": self.scan_secrets(),
            "permission_issues": self.check_file_permissions(),
            "recommendations": []
        }

        # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        if report["permission_issues"]:
            report["recommendations"].append("Fix file permission issues")

        if "vulnerabilities" in report["dependency_scan"]:
            report["recommendations"].append("Update vulnerable dependencies")

        return report
```

### 2. ì ‘ê·¼ ì œì–´ ê´€ë¦¬

#### API í‚¤ ë¡œí…Œì´ì…˜
```python
# security/key_rotation.py
import os
import hashlib
from datetime import datetime, timedelta

class KeyRotationManager:
    """API í‚¤ ë¡œí…Œì´ì…˜ ê´€ë¦¬"""

    def __init__(self):
        self.key_file = "/secure/api_keys.json"

    def should_rotate_key(self, key_name):
        """í‚¤ ë¡œí…Œì´ì…˜ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        key_info = self.get_key_info(key_name)
        if not key_info:
            return True

        created_date = datetime.fromisoformat(key_info["created"])
        return datetime.now() - created_date > timedelta(days=90)

    def rotate_anthropic_key(self):
        """Anthropic API í‚¤ ë¡œí…Œì´ì…˜"""
        if self.should_rotate_key("anthropic"):
            print("ğŸ”‘ Rotating Anthropic API key...")

            # 1. ìƒˆ í‚¤ ë°œê¸‰ (ìˆ˜ë™ ê³¼ì •)
            print("Please generate a new API key from Anthropic Console")
            new_key = input("Enter new API key: ")

            # 2. ìƒˆ í‚¤ ê²€ì¦
            if self.validate_anthropic_key(new_key):
                # 3. í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                self.update_env_file("ANTHROPIC_API_KEY", new_key)

                # 4. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
                os.system("systemctl restart kakao-template")

                print("âœ… Key rotation completed")
            else:
                print("âŒ Key validation failed")

    def validate_anthropic_key(self, api_key):
        """API í‚¤ ìœ íš¨ì„± ê²€ì¦"""
        try:
            from anthropic import Anthropic
            client = Anthropic(api_key=api_key)
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
            return True
        except Exception:
            return False
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### 1. ì •ê¸° ì„±ëŠ¥ ë¶„ì„

#### ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
```python
# performance/profiler.py
import cProfile
import pstats
import io
from datetime import datetime

class PerformanceProfiler:
    """ì„±ëŠ¥ í”„ë¡œíŒŒì¼ëŸ¬"""

    def profile_template_generation(self, num_requests=100):
        """í…œí”Œë¦¿ ìƒì„± ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§"""
        from src.workflow.langgraph_workflow import TemplateGenerationWorkflow

        workflow = TemplateGenerationWorkflow()

        # í”„ë¡œíŒŒì¼ë§ ì‹œì‘
        pr = cProfile.Profile()
        pr.enable()

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        for i in range(num_requests):
            workflow.run({
                "user_request": f"í…ŒìŠ¤íŠ¸ ìš”ì²­ {i}",
                "business_type": "êµìœ¡"
            })

        pr.disable()

        # ê²°ê³¼ ë¶„ì„
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
        ps.print_stats()

        # ë¦¬í¬íŠ¸ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        with open(f'performance/profile_{timestamp}.txt', 'w') as f:
            f.write(s.getvalue())

        return s.getvalue()

    def analyze_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        import tracemalloc
        import gc

        tracemalloc.start()

        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹¤í–‰
        from src.database.vector_store import PolicyVectorStore
        vs = PolicyVectorStore()

        # ì—¬ëŸ¬ ê²€ìƒ‰ ì‹¤í–‰
        for i in range(100):
            vs.search_relevant_policies(f"ê²€ìƒ‰ì–´ {i}", k=5)

        # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
        snapshot = tracemalloc.take_snapshot()
        top_stats = snapshot.statistics('lineno')

        print("Top 10 memory consumers:")
        for stat in top_stats[:10]:
            print(stat)

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì •ë³´
        gc_stats = {
            "generation_0": gc.get_count()[0],
            "generation_1": gc.get_count()[1],
            "generation_2": gc.get_count()[2],
            "collected": gc.collect()
        }

        return gc_stats
```

### 2. ìºì‹œ ìµœì í™”

#### ì§€ëŠ¥í˜• ìºì‹± ì „ëµ
```python
# performance/cache_optimizer.py
import redis
import json
import hashlib
from datetime import datetime, timedelta

class CacheOptimizer:
    """ìºì‹œ ìµœì í™” ê´€ë¦¬ì"""

    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)

    def analyze_cache_usage(self):
        """ìºì‹œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        cache_stats = {
            "total_keys": self.redis_client.dbsize(),
            "memory_usage": self.redis_client.info('memory')['used_memory_human'],
            "hit_ratio": self.calculate_hit_ratio(),
            "popular_keys": self.get_popular_keys(),
            "expired_keys": self.count_expired_keys()
        }

        return cache_stats

    def optimize_cache_ttl(self):
        """ìºì‹œ TTL ìµœì í™”"""
        # ì‚¬ìš© ë¹ˆë„ì— ë”°ë¥¸ TTL ì¡°ì •
        key_patterns = {
            "policy:*": 3600,      # 1ì‹œê°„
            "template:*": 1800,    # 30ë¶„
            "analysis:*": 900,     # 15ë¶„
            "temp:*": 300          # 5ë¶„
        }

        for pattern, ttl in key_patterns.items():
            keys = self.redis_client.keys(pattern)
            for key in keys:
                current_ttl = self.redis_client.ttl(key)
                if current_ttl == -1:  # ë§Œë£Œ ì‹œê°„ì´ ì—†ëŠ” í‚¤
                    self.redis_client.expire(key, ttl)

    def cleanup_unused_cache(self):
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìºì‹œ ì •ë¦¬"""
        # 30ì¼ ì´ìƒ ì ‘ê·¼í•˜ì§€ ì•Šì€ í‚¤ ì‚­ì œ
        cutoff_date = datetime.now() - timedelta(days=30)

        for key in self.redis_client.scan_iter():
            last_access = self.redis_client.object('idletime', key)
            if last_access and last_access > 30 * 24 * 3600:  # 30ì¼
                self.redis_client.delete(key)
```

## ğŸ“‹ ì •ê¸° ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸
```markdown
## ğŸ“… ì¼ì¼ ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œìŠ¤í…œ ìƒíƒœ
- [ ] ì„œë¹„ìŠ¤ ì •ìƒ ì‹¤í–‰ í™•ì¸ (curl http://localhost:8000/health)
- [ ] CPU ì‚¬ìš©ë¥  < 80%
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  < 85%
- [ ] ë””ìŠ¤í¬ ì‚¬ìš©ë¥  < 90%

### ë¡œê·¸ ëª¨ë‹ˆí„°ë§
- [ ] ì—ëŸ¬ ë¡œê·¸ í™•ì¸ (ERROR ë ˆë²¨)
- [ ] ê²½ê³  ë¡œê·¸ ê²€í†  (WARN ë ˆë²¨)
- [ ] ì„±ëŠ¥ ì´ìŠˆ ë¡œê·¸ í™•ì¸

### API ì„±ëŠ¥
- [ ] í‰ê·  ì‘ë‹µ ì‹œê°„ < 3ì´ˆ
- [ ] ì—ëŸ¬ìœ¨ < 1%
- [ ] ì²˜ë¦¬ëŸ‰ ì •ìƒ ë²”ìœ„ ë‚´

### ë³´ì•ˆ
- [ ] ë¹„ì •ìƒì ì¸ ì ‘ê·¼ ì‹œë„ í™•ì¸
- [ ] API í‚¤ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
```

### ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸
```markdown
## ğŸ“… ì£¼ê°„ ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°±ì—…
- [ ] ìë™ ë°±ì—… ì •ìƒ ì‹¤í–‰ í™•ì¸
- [ ] ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
- [ ] ë°±ì—… ì €ì¥ì†Œ ìš©ëŸ‰ í™•ì¸

### ì„±ëŠ¥ ë¶„ì„
- [ ] ì£¼ê°„ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
- [ ] ì‘ë‹µ ì‹œê°„ íŒ¨í„´ ê²€í† 
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì´ í™•ì¸

### ë³´ì•ˆ ì ê²€
- [ ] ì˜ì¡´ì„± ë³´ì•ˆ ìŠ¤ìº” ì‹¤í–‰
- [ ] ì ‘ê·¼ ë¡œê·¸ ë¶„ì„
- [ ] ì‹œìŠ¤í…œ íŒ¨ì¹˜ í™•ì¸

### ìš©ëŸ‰ ê´€ë¦¬
- [ ] ë¡œê·¸ íŒŒì¼ ì •ë¦¬
- [ ] ì„ì‹œ íŒŒì¼ ì •ë¦¬
- [ ] ìºì‹œ ìµœì í™”
```

### ì›”ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸
```markdown
## ğŸ“… ì›”ê°„ ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì—…ë°ì´íŠ¸
- [ ] ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸ ê²€í† 
- [ ] ë³´ì•ˆ íŒ¨ì¹˜ ì ìš©
- [ ] ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸

### ì„±ëŠ¥ ìµœì í™”
- [ ] ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
- [ ] ë³‘ëª© ì§€ì  ë¶„ì„
- [ ] ìµœì í™” ê³„íš ìˆ˜ë¦½

### ë°±ì—… ë° ë³µêµ¬
- [ ] ë°±ì—… ë³µêµ¬ í…ŒìŠ¤íŠ¸
- [ ] ì¬í•´ ë³µêµ¬ ê³„íš ê²€í† 
- [ ] ë°±ì—… ì •ì±… í‰ê°€

### ìš©ëŸ‰ ê³„íš
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
- [ ] ìŠ¤ì¼€ì¼ë§ ê³„íš ìˆ˜ë¦½
- [ ] ì¸í”„ë¼ ë¹„ìš© ë¶„ì„
```

---

**ğŸ“… ì‘ì„±ì¼**: 2024ë…„ 9ì›” 19ì¼
**âœï¸ ì‘ì„±ì**: Final Team 3 AI
**ğŸ“„ ë¬¸ì„œ ë²„ì „**: 1.0
**ğŸ”„ ìµœì¢… ìˆ˜ì •**: 2024ë…„ 9ì›” 19ì¼